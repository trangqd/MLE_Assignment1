{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca729169-9ea5-4441-88c2-77444a19693c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/15 13:43:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                "
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'datetime' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m start_date_str = datetime.strptime(df_dates[\u001b[33m\"\u001b[39m\u001b[33mmin_dt\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33m%\u001b[39m\u001b[33mY-\u001b[39m\u001b[33m%\u001b[39m\u001b[33mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m\"\u001b[39m).strftime(\u001b[33m\"\u001b[39m\u001b[33m%\u001b[39m\u001b[33mY-\u001b[39m\u001b[33m%\u001b[39m\u001b[33mm-01\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m end_date_str   = datetime.strptime(df_dates[\u001b[33m\"\u001b[39m\u001b[33mmax_dt\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33m%\u001b[39m\u001b[33mY-\u001b[39m\u001b[33m%\u001b[39m\u001b[33mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m\"\u001b[39m).replace(day=\u001b[32m1\u001b[39m).strftime(\u001b[33m\"\u001b[39m\u001b[33m%\u001b[39m\u001b[33mY-\u001b[39m\u001b[33m%\u001b[39m\u001b[33mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m dates = \u001b[43mgenerate_first_of_month_dates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_date_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_date_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBack-filling Bronze for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dates)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m months: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdates[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m → \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdates[-\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     21\u001b[39m process_bronze(dates, spark)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/utils/bronze_processor.py:20\u001b[39m, in \u001b[36mgenerate_first_of_month_dates\u001b[39m\u001b[34m(start_str, end_str)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_first_of_month_dates\u001b[39m(start_str: \u001b[38;5;28mstr\u001b[39m, end_str: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     start = \u001b[43mdatetime\u001b[49m.strptime(start_str, \u001b[33m\"\u001b[39m\u001b[33m%\u001b[39m\u001b[33mY-\u001b[39m\u001b[33m%\u001b[39m\u001b[33mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     21\u001b[39m     end   = datetime.strptime(end_str,   \u001b[33m\"\u001b[39m\u001b[33m%\u001b[39m\u001b[33mY-\u001b[39m\u001b[33m%\u001b[39m\u001b[33mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     22\u001b[39m     dates = []\n",
      "\u001b[31mNameError\u001b[39m: name 'datetime' is not defined"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from utils.bronze_processor import generate_first_of_month_dates, process_bronze\n",
    "from utils.spark import get_spark\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spark = get_spark(\"assignment1-bronze\")\n",
    "\n",
    "    df_dates = (\n",
    "        spark.read.option(\"header\", \"true\")\n",
    "             .csv(\"data/raw/lms_loan_daily.csv\")\n",
    "             .selectExpr(\"min(snapshot_date) as min_dt\", \"max(snapshot_date) as max_dt\")\n",
    "             .collect()[0]\n",
    "    )\n",
    "\n",
    "    start_date_str = datetime.strptime(df_dates[\"min_dt\"], \"%Y-%m-%d\").strftime(\"%Y-%m-01\")\n",
    "    end_date_str   = datetime.strptime(df_dates[\"max_dt\"], \"%Y-%m-%d\").replace(day=1).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    dates = generate_first_of_month_dates(start_date_str, end_date_str)\n",
    "    print(f\"Back-filling Bronze for {len(dates)} months: {dates[0]} → {dates[-1]}\")\n",
    "\n",
    "    process_bronze(dates, spark)\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9fbb42-b34d-4140-8cf5-26d7622e551f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
